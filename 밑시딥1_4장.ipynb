{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4장.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1BQFj-9eolVq7QviV0zQ_XuxF5w6QrDxx",
      "authorship_tag": "ABX9TyPRp2psu2qLnZ5ndJAGsrEm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/du6293/DL_study/blob/main/4%EC%9E%A5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 교차 엔트로피 오차"
      ],
      "metadata": {
        "id": "wvFYTpuN2Zbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "R0TaZnRJ2s4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 오차제곱합\n",
        "def sum_squares_error(y,t):\n",
        "  return 0.5 * np.sum((y-t)**2)"
      ],
      "metadata": {
        "id": "oxkS3vHQnfkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9Z1QsMw2Nt0"
      },
      "outputs": [],
      "source": [
        "# 교차 엔트로피 오차\n",
        "def cross_entropy_error(y,t):  # y와 t는 넘파이 배열\n",
        "  delta = 1e-7\n",
        "  return -np.sum(t * np.log(y+ delta))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MNIST 미니배치 학습 (데이터의 일부를 추려 전체의 근사치로 이용)"
      ],
      "metadata": {
        "id": "FXcm8DAioVtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "import numpy as np\n",
        "from dataset.mnist import load_mnist\n",
        "\n",
        "(x_train, x_test),(x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
        "print(x_train.shape) # (60000,784)\n",
        "print(t_train.shape) # (60000,10)"
      ],
      "metadata": {
        "id": "towu1N-ioVal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = x_train.shape[0]\n",
        "batch_size = 10\n",
        "batch_mask = np.random.choice(train_size, batch_size) # train데이터에서 무작위로 10장 빼냄>>미니배치로 뽑아낼 인덱스로 활용"
      ],
      "metadata": {
        "id": "h6gw6grApDHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#(배치용) 교차 엔트로피 오차 구현(정답 레이블 t가 one-hot-encoding일 때)\n",
        "def cross_entropy_error(y,t):  # y: 신경망의 출력, t : 정답 레이블\n",
        "  if y.ndim == 1:\n",
        "    t = t.reshape(1,t.size)  # reshape : 데이터의 형상을 바꿈\n",
        "    y = y.reshape(1,y.size)\n",
        "  batch_size = y.shape[0]\n",
        "  return -np.sum(t*np.log(y+1e-7)) / batch_size  # 배치의 크기로 나누어 정규화한 후 1장당 평균의 교차 엔트로피 오차를 계산"
      ],
      "metadata": {
        "id": "XxeeiyaSpZvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#(배치용) 교차 엔트로피 오차 구현(정답 레이블 t가 숫자 레이블일 때)\n",
        "def cross_entropy_error(y,t):  # y: 신경망의 출력, t : 정답 레이블\n",
        "  if y.ndim == 1:\n",
        "    t = t.reshape(1,t.size)  # reshape : 데이터의 형상을 바꿈\n",
        "    y = y.reshape(1,y.size)\n",
        "  batch_size = y.shape[0]\n",
        "  return -np.sum(np.log(y[np.arrange(batch_size),t] + 1e-7)) / batch_size"
      ],
      "metadata": {
        "id": "u7_KVM9JqGfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 수치 미분"
      ],
      "metadata": {
        "id": "tR35Prpxq8Oc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 수치미분의 나쁜 구현 예  > 오차발생\n",
        "def numerical_diff(f,x):\n",
        "  h = 10e-50\n",
        "  return (f(x+h)-f(x)) / h\n"
      ],
      "metadata": {
        "id": "xUafJ7_xq3F3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 수치미분의 올바른 구현 예\n",
        "def numerical_diff(f,x):\n",
        "  h = 1e-4  # 0.0001\n",
        "  return (f(x+h) - f(x-h)) / (2 * h)"
      ],
      "metadata": {
        "id": "pRMiqDZUraLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y = 0.01x2 + 0.1x 구현\n",
        "def function_1(x):\n",
        "  return 0.01 * x ** 2 + 0.1 * x"
      ],
      "metadata": {
        "id": "anpgIa1F8xod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "x = np.arange(0.0, 20.0, 0.1) # 0에서 20까지 0.1 간격의 배열 x를 만든다.\n",
        "y = function_1(x)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"f(x)\")\n",
        "plt.plot(x,y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BkGaKR998-b0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 편미분\n",
        "변수가 여럿인 함수에 대한 미분"
      ],
      "metadata": {
        "id": "vvE2qYxY9rev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def function_2(x):\n",
        "  return x[0] ** 2 + x[1] ** 2\n",
        "  # 또는 return np.sum(x**2)\n",
        "  "
      ],
      "metadata": {
        "id": "PagzcsOC9hh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 기울기"
      ],
      "metadata": {
        "id": "CZNi7CWQ-o3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def numerical_gradient(f,x):\n",
        "  h = 1e-4 # 0.0001\n",
        "  grad = np.zeros_like(x) # x와 형상이 같고 원소가 모두 0인 배열을 생성\n",
        "\n",
        "  for idx in range(x.size):\n",
        "    tmp_val = x[idx]\n",
        "    # f(x+h) 계산\n",
        "    x[idx] = tmp_val + h\n",
        "    fxh1 = f(x)\n",
        "\n",
        "    # f(x-h) 계산\n",
        "    x[idx] = tmp_val - h\n",
        "    fxh2 = f(x)\n",
        "\n",
        "    grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
        "    x[idx] = tmp_val  # 값 복원\n",
        "\n",
        "  return grad"
      ],
      "metadata": {
        "id": "IWaACDxA-rQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(numerical_gradient(function_2,np.array([3.0,4.0])))\n",
        "print(numerical_gradient(function_2,np.array([0.0,2.0])))\n",
        "print(numerical_gradient(function_2,np.array([3.0,0.0])))"
      ],
      "metadata": {
        "id": "auEYgagA_zis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 경사 하강법"
      ],
      "metadata": {
        "id": "kghQFfdiAoUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(f, init_x, lr = 0.01, step_num = 100):  # lr : 학습률, step_num = 반복횟수\n",
        "  x = init_x\n",
        "\n",
        "  for i in range(step_num):\n",
        "    grad = numerical_gradient(f,x)  # 기울기를 구함\n",
        "    x -= lr * grad  # 기울기 * 학습률\n",
        "  return x"
      ],
      "metadata": {
        "id": "ZeCZsuarAq-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 경사법으로 f(x0,x1) = x02+x12의 최솟값을 구하기\n",
        "def fucntion_2(x):\n",
        "  return x[0] ** 2 + x[1] ** 2\n",
        "\n",
        "init_x = np.array([-3.0, 4.0])\n",
        "gradient_descent(function_2, init_x = init_x, lr = 0.1, step_num = 100)"
      ],
      "metadata": {
        "id": "Rb00eydwBkJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습률이 너무 큰 경우 : lr = 10.0\n",
        "init_x = np.array([-3.0, 4.0])\n",
        "gradient_descent(function_2, init_x = init_x, lr = 10.0, step_num = 100)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GpRLLw75CL2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습률이 너무 작은 경우 : lr = 1e-10\n",
        "init_x = np.array([-3.0,4.0])\n",
        "gradient_descent(function_2, init_x = init_x, lr = 1e-10, step_num = 100)"
      ],
      "metadata": {
        "id": "iPWHgQRwCp1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 신경망에서의 기울기\n"
      ],
      "metadata": {
        "id": "_EkzLEoDDOgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 기울기를 구하는 코드\n",
        "import sys, os\n",
        "sys.path.append(\"/content/drive/MyDrive/bottom/deep-learning-from-scratch-master/deep-learning-from-scratch-master/common\")\n",
        "import numpy as np\n",
        "from common.functions import softmax, cross_entropy_error\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "class simpleNet:\n",
        "  def __init__(self):\n",
        "    self.w = np.random.randn(2,3)  # 정규분포로 초기화\n",
        "\n",
        "  def predict(self, x):\n",
        "    return np.dot(x,self.w)\n",
        "\n",
        "  def loss(self, x, t):\n",
        "    z = self.predict(x)\n",
        "    y = softmax(z)\n",
        "    loss = cross_entropy_error(y,t)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "wmuw7-7UDQaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = simpleNet()\n",
        "print(net,w)  # 가중치 매개변수\n",
        "x = np.array([0.6, 0.9])\n",
        "\n",
        "p = net.predict(x)\n",
        "print(p)\n",
        "\n",
        "np.argmax(p)\n",
        "\n",
        "t = np.array([0,0,1]) # 정답 레이블\n",
        "net.loss(x,t)"
      ],
      "metadata": {
        "id": "7ZWCncMpFZS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2층 신경망 클래스 구현하기"
      ],
      "metadata": {
        "id": "5t69EaLHGFzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os \n",
        "sys.path.append(os.pardir)\n",
        "from common.functions import *\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "class TwoLayerNet:\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
        "    # 가중치 초기화\n",
        "    self.params = {}  # 신경망의 매개변수를 보관하는 딕셔너리 변수\n",
        "    self.params['w1'] = weight_init_std * np.random.randn(input_size, hidden_size)  # 1번째 층의 가중치\n",
        "    self.params['b1'] = np.zeros(hidden_size)                                       # 1번째 층의 편향\n",
        "    self.params['w2'] = weight_init_std * np.random.randn(hidden_size, output_size) # 2번째 층의 가중치\n",
        "    self.params['b2'] = np.zeros(output_size)                                       # 2번째 층의 편향\n",
        "\n",
        "    b1, b2 = self.params['b1'], self.params['b2']\n",
        "\n",
        "    a1 = np.dot(x,w1) + b1\n",
        "    z1 = sigmoid(a1)\n",
        "    z2 = np.dot(z1,w2) + b2\n",
        "    y = softmax(z2)\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def loss(self, x, t):\n",
        "      y = self.predict(x)\n",
        "      return cross_entropy_error(y,t)\n",
        "\n",
        "    def accuracy(self, x, t):\n",
        "      y = self.predict(x)\n",
        "      y = np.argmax(y, axis = 1)\n",
        "      y = np.argmax(t, axis = 1)\n",
        "\n",
        "      accuracy = np.sum(y==t) / float(x.shape[0])\n",
        "      return accuracy\n",
        "      \n",
        "      # x:입력 데이터, t: 정답 레이블\n",
        "    def numerical_gradient(self,x,t):\n",
        "      loss_w = lambda w: self.loss(x,t)\n",
        "\n",
        "      grads = {}  # 기울기를 보관하는 딕셔너리 변수\n",
        "      grads['w1'] = numerical_gradient(loss_w, self.params['w1'])  # 1번째 층의 가중치의 기울기\n",
        "      grads['b1'] = numerical_gradient(loss_w, self.params['b1'])  # 1번째 층의 편향의 기울기\n",
        "      grads['w3'] = numerical_gradient(loss_w, self.params['w2'])  # 2번재 층의 가중치의 기울기\n",
        "      grads['b2'] = numerical_gradient(loss_w, self.params['b2'])  # 2번째 층의 편향의 기울기\n",
        "\n",
        "      return grads"
      ],
      "metadata": {
        "id": "fGUYG-O4GFiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 미니 배치 학습 구현하기\n"
      ],
      "metadata": {
        "id": "XnAW3_--J7lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from dataset.mnist import load_mnist\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_encoding = True)\n",
        "\n",
        "train_loss_list = []\n",
        "\n",
        "# 하이퍼파라미터\n",
        "iters_num = 10000 # 반복횟수\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100 # 미니 배치 크기\n",
        "learning_rate = 0.1\n",
        "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n",
        "\n",
        "for i in range(iters_num):\n",
        "  # 미니배치 획득\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\n",
        "  x_batch = x_train[batch_mask]\n",
        "  t_batch = t_train[batch_mask]\n",
        "\n",
        "  # 기울기 계산 \n",
        "  grad = network.numerical_gradient(x_batch, t_batch)\n",
        "  # grad = network.gradient(x_batch, t_batch)  # 성능 개선판 !\n",
        "\n",
        "  # 매개변수 갱신\n",
        "  for key in ('w1','b1','w2','b2'):\n",
        "    network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "  # 학습 경과 기록\n",
        "  loss = network.loss(x_batch, t_batch)\n",
        "  train_loss_list.append(loss)\n",
        "\n"
      ],
      "metadata": {
        "id": "BtKFD7frJ-mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시험 데이터로 평가하기\n",
        "import numpy as np\n",
        "from dataset.mnist import load_mnist\n",
        "from two_layer_net import TwoLayerNet\n",
        "(x_train, t_train),(x_test,t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
        "\n",
        "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n",
        "\n",
        "# 하이퍼 파라미터\n",
        "iters_num = 10000 # 반복 횟수를 적절히 설정\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100 # 미니배치 크기\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list =[]\n",
        "\n",
        "# 1에폭 당 반복 수\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "  # 미니배치 획득\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\n",
        "  x_batch = x_train[batch_mask]\n",
        "  t_batch = t_train[batch_mask]\n",
        "\n",
        "  # 기울기 계산\n",
        "  grad = network.numerical_gradient(x_batch, t_batch)\n",
        "  # grad = network.gradient(x_batch, t_batch) # 성능 개선판 !\n",
        "\n",
        "  # 매개변수 갱신\n",
        "  for key in ('w1','b1','w2','b2'):\n",
        "    network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "  # 학습 경과 기록\n",
        "  loss = network.loss(x_batch, t_batch)\n",
        "  train_loss_list.append(loss)\n",
        "\n",
        "  # 1에폭 당 정확도 계산\n",
        "  if i % iter_per_epoch == 0:\n",
        "    train_acc = network.accuracy(x_train, t_train)\n",
        "    test_acc = network.accuracy(x_test, t_test)\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print(\"train acc, test acc | \" + str(train_acc) + \" , \" + str(test_acc))"
      ],
      "metadata": {
        "id": "uHYe6k_bQiRT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
