{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6장.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1asdbCV60qSrvTAmq_BJEY7MHZd76CyT5",
      "authorship_tag": "ABX9TyOYQ/cnt77DxuAYUOUvc9JP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/du6293/DL_study/blob/main/6%EC%9E%A5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3K4QIG3oHpkk"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습 관련 기술들\n"
      ],
      "metadata": {
        "id": "CuM-iHEBJYKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 확률적 경사 하강법(SGD)"
      ],
      "metadata": {
        "id": "l6G6Q4QqJg7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "  def __init__(self, lr = 0.01)\n",
        "    self.lr = lr\n",
        "\n",
        "  def update(self, params, grads):\n",
        "    for key in params.keys():\n",
        "      params[key] -= self.lr * grads[key]"
      ],
      "metadata": {
        "id": "eDA6lCGHJcii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 모멘텀"
      ],
      "metadata": {
        "id": "VA7L4LuJLkto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Momentum:\n",
        "  def __init__(self, lr = 0.01, momentum = 0.9)\n",
        "    self.lr = lr\n",
        "    self.momentum = momentum\n",
        "    self.v = None  # 속도\n",
        "  def update(self, params, grads):\n",
        "    if self.v is None:\n",
        "      self.v = {}\n",
        "      for key , val in params.items():\n",
        "        self.v[key] = np.zeros_like(val)\n",
        "      \n",
        "    for key in params.keys():\n",
        "      self.v[key] = self.momentum * self.v[key] - self.lr * grad[key]\n",
        "      params[key] += self.v[key]"
      ],
      "metadata": {
        "id": "B5BU5dh6LnQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. AdaGrad"
      ],
      "metadata": {
        "id": "AhgBxmweQuh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaGrad:\n",
        "  def __init__(self, lr= 0.01):\n",
        "    self.lr = lr\n",
        "    self.h = None\n",
        "  \n",
        "  def update(self, params, grads):\n",
        "    if self.h is None:\n",
        "      self.h = {}\n",
        "      \n",
        "      for key, val in params.items():\n",
        "        self.h[key] = np.zeros_like(val)\n",
        "    \n",
        "    for key in params.keys():\n",
        "      self.h[key] += grads[key] * grads[key]\n",
        "      prarams[key] -= self.lr * grads[key] / (np.sqrt(self.h[key])) + le-7  # self.h[key]에 0이 담겨져 있다고 해도 0으로 나누는 사태를 막음"
      ],
      "metadata": {
        "id": "kR3_rmX7QxDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. RMSProp"
      ],
      "metadata": {
        "id": "RySBpwt3ULbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSProp:\n",
        "  def __init__ (self, lr = 0.01, decay_rate = 0.99 )\n",
        "    self.lr = lr\n",
        "    self.decay_rate = decay_rate\n",
        "    self.h = None\n",
        "  \n",
        "  def update(self, params, grads):\n",
        "    if self.h is None:\n",
        "      self.h = {}\n",
        "      \n",
        "      for key, val in params,items():\n",
        "        self.h[key] = np.zeros_like(val)\n",
        "\n",
        "    for key in params.keys():\n",
        "      self.h[key] *= self.decay_rate\n",
        "      self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
        "      params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
        "\n"
      ],
      "metadata": {
        "id": "olKsyzo9UPvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Adam"
      ],
      "metadata": {
        "id": "BHo4d1FpSgJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Adam:  # 모멘텀 + AdaGrad\n",
        "  def __init__(self, lr = 0.01, beta = 0.9, beta2 = 0.999 ):\n",
        "    self.lr = lr\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "    self.iter = 0\n",
        "    self.m = None\n",
        "    self.v = None\n",
        "\n",
        "  def update(self, params, grads):\n",
        "    if self.m is None:\n",
        "      self.m, self.v = {},{}\n",
        "      for key, val in params.items():\n",
        "        self.m[key] = np.zeros_like(val)\n",
        "        self.v[key] = np.zeros_like(val)\n",
        "\n",
        "    self.iter += 1\n",
        "    lr_t = self.lr * np.sqrt(1.0 - self.beta2 ** self.iter) / (1.0 - self.beta1 ** self.iter)\n",
        "\n",
        "\n",
        "    for key in params.keys():\n",
        "      self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
        "      self.v[key] += (1 - self.beta2) * (grads[key] ** 2 - self.v[key])\n",
        "\n",
        "      params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e - 7) \n"
      ],
      "metadata": {
        "id": "SBTKy6KnSin4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MNIST 데이터셋으로 본 갱신 방법 비교"
      ],
      "metadata": {
        "id": "KQ1YKyxXYLxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "sys.path.append(\"/content/drive/MyDrive/bottom/deep-learning-from-scratch-master/deep-learning-from-scratch-master/ch06\")  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.util import smooth_curve\n",
        "from common.multi_layer_net import MultiLayerNet\n",
        "from common.optimizer import *\n",
        "\n",
        "\n",
        "# 0. MNIST 데이터 읽기==========\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 128\n",
        "max_iterations = 2000\n",
        "\n",
        "\n",
        "# 1. 실험용 설정==========\n",
        "optimizers = {}\n",
        "optimizers['SGD'] = SGD()\n",
        "optimizers['Momentum'] = Momentum()\n",
        "optimizers['AdaGrad'] = AdaGrad()\n",
        "optimizers['Adam'] = Adam()\n",
        "#optimizers['RMSprop'] = RMSprop()\n",
        "\n",
        "networks = {}\n",
        "train_loss = {}\n",
        "for key in optimizers.keys():\n",
        "    networks[key] = MultiLayerNet(\n",
        "        input_size=784, hidden_size_list=[100, 100, 100, 100],\n",
        "        output_size=10)\n",
        "    train_loss[key] = []    \n",
        "\n",
        "\n",
        "# 2. 훈련 시작==========\n",
        "for i in range(max_iterations):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    for key in optimizers.keys():\n",
        "        grads = networks[key].gradient(x_batch, t_batch)\n",
        "        optimizers[key].update(networks[key].params, grads)\n",
        "    \n",
        "        loss = networks[key].loss(x_batch, t_batch)\n",
        "        train_loss[key].append(loss)\n",
        "    \n",
        "    if i % 100 == 0:\n",
        "        print( \"===========\" + \"iteration:\" + str(i) + \"===========\")\n",
        "        for key in optimizers.keys():\n",
        "            loss = networks[key].loss(x_batch, t_batch)\n",
        "            print(key + \":\" + str(loss))\n",
        "\n",
        "\n",
        "# 3. 그래프 그리기==========\n",
        "markers = {\"SGD\": \"o\", \"Momentum\": \"x\", \"AdaGrad\": \"s\", \"Adam\": \"D\"}\n",
        "x = np.arange(max_iterations)\n",
        "for key in optimizers.keys():\n",
        "    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oU0M4s3LYKGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **은닉층의 활성화값 분포**"
      ],
      "metadata": {
        "id": "nqF4v0G8dOWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "가중치의 초깃값에 따른 은닉층 활성화값(활성화 함수의 출력 데이터)들의 변화"
      ],
      "metadata": {
        "id": "9VtUM5T2dfsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5층 신경망에 무작위로 생성한 입력 데이터를 흘리며 각 층의 활성화값 분포를 히스토그램으로 그리기"
      ],
      "metadata": {
        "id": "tQf2Db6Wfpqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.표준편차를 1로"
      ],
      "metadata": {
        "id": "zoR9K2cVjuW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def ReLU(x):\n",
        "  return np.maximum(0,x)\n",
        "  \n",
        "def tanh(x):\n",
        "  return np.tang(x)\n",
        "\n",
        "\n",
        "input_data = np.random.randn(1000,100)  # 1000개의 데이터 >> 정규분포로 무작위로 생성헤 5층 신경망에 흘림\n",
        "node_num = 100 # 각 은닉층의 노드(뉴런) 수\n",
        "hidden_layer_size = 5  # 은닉층이 5개\n",
        "activations = {}  # 이곳에 각 층의 활성화 결과를 저장\n",
        "\n",
        "x = input_data\n",
        "\n",
        "for i in range(hidden_layer_size):\n",
        "  if i != 0:\n",
        "    x = activations[i-1]\n",
        "\n",
        "  # 초깃값을 다양하게 바꿔가며 실험해보자!\n",
        "  w = np.random.randn(node_num, node_num) * 1\n",
        "  # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
        "  # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
        "  a = np.dot(x,w)\n",
        "\n",
        "\n",
        "  #활성화 함수도 바꿔가며 실험해보자\n",
        "  z = sigmoid(a)\n",
        "  # z = ReLU(a)\n",
        "  # z = tanh(a)\n",
        "\n",
        "\n",
        "  activations[i] = z"
      ],
      "metadata": {
        "id": "IMprXZAKdVhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 히스토그램 그리기\n",
        "for i , a in activations.items():\n",
        "  plt.subplot(1,len(activations), i+1)\n",
        "  plt.title(str(i+1) + \"layer\")\n",
        "  if i != 0 :\n",
        "    plt.yticks([],[])\n",
        "    plt.hist(a.flatten(), 30, range=(0,1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U-7MzwqrcG4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "기울기 소실 >> 딥러닝에서 심각한 문제가 된다.\n",
        "각 층의 활성화 값들이 0과 1에 치우쳐 분포\n",
        "시그모이드 함수는 출력이 0과 1에 가까워지자 미분이 0에 다가간다.\n",
        "따라서 기울기값이 0과 1에 치우쳐 분포하게 되면 역전파의 기울기 값이 점점 작아지다가 사라진다."
      ],
      "metadata": {
        "id": "t615ZcgEjHN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.표준편차를 0.01로"
      ],
      "metadata": {
        "id": "ZO4ipmPpjqUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def ReLU(x):\n",
        "  return np.maximum(0,x)\n",
        "  \n",
        "def tanh(x):\n",
        "  return np.tang(x)\n",
        "\n",
        "\n",
        "input_data = np.random.randn(1000,100)  # 1000개의 데이터 >> 정규분포로 무작위로 생성헤 5층 신경망에 흘림\n",
        "node_num = 100 # 각 은닉층의 노드(뉴런) 수\n",
        "hidden_layer_size = 5  # 은닉층이 5개\n",
        "activations = {}  # 이곳에 각 층의 활성화 결과를 저장\n",
        "\n",
        "x = input_data\n",
        "\n",
        "for i in range(hidden_layer_size):\n",
        "  if i != 0:\n",
        "    x = activations[i-1]\n",
        "\n",
        "  # 초깃값을 다양하게 바꿔가며 실험해보자!\n",
        "  w = np.random.randn(node_num, node_num) * 0.01\n",
        "  # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
        "  # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
        "  a = np.dot(x,w)\n",
        "\n",
        "\n",
        "  #활성화 함수도 바꿔가며 실험해보자\n",
        "  z = sigmoid(a)\n",
        "  # z = ReLU(a)\n",
        "  # z = tanh(a)\n",
        "\n",
        "\n",
        "  activations[i] = z"
      ],
      "metadata": {
        "id": "YvckmklFjkxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 히스토그램 그리기\n",
        "for i , a in activations.items():\n",
        "  plt.subplot(1,len(activations), i+1)\n",
        "  plt.title(str(i+1) + \"layer\")\n",
        "  if i != 0 :\n",
        "    plt.yticks([],[])\n",
        "    plt.hist(a.flatten(), 30, range=(0,1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tvs_b5Rgj2-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "표현력 제한\n",
        "0.5부근에 활성화값들이 집중됨 >> 다수의 뉴런이 거의 같은 값을 출력하고 있으니 뉴런을 여러 개 둔 의미가 없어진다."
      ],
      "metadata": {
        "id": "_f6YZrO_kQww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Xavier초깃값 - sigmoid함수"
      ],
      "metadata": {
        "id": "Pr4lmrFnk1_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def ReLU(x):\n",
        "  return np.maximum(0,x)\n",
        "  \n",
        "def tanh(x):\n",
        "  return np.tang(x)\n",
        "\n",
        "\n",
        "input_data = np.random.randn(1000,100)  # 1000개의 데이터 >> 정규분포로 무작위로 생성헤 5층 신경망에 흘림\n",
        "node_num = 100 # 각 은닉층의 노드(뉴런) 수\n",
        "hidden_layer_size = 5  # 은닉층이 5개\n",
        "activations = {}  # 이곳에 각 층의 활성화 결과를 저장\n",
        "\n",
        "x = input_data\n",
        "\n",
        "for i in range(hidden_layer_size):\n",
        "  if i != 0:\n",
        "    x = activations[i-1]\n",
        "\n",
        "  # 초깃값을 다양하게 바꿔가며 실험해보자!\n",
        "  w = np.random.randn(node_num, node_num) / np.sqrt(node_num)\n",
        "  # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
        "  # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
        "  a = np.dot(x,w)\n",
        "\n",
        "\n",
        "  #활성화 함수도 바꿔가며 실험해보자\n",
        "  z = sigmoid(a)\n",
        "  # z = ReLU(a)\n",
        "  # z = tanh(a)\n",
        "\n",
        "\n",
        "  activations[i] = z"
      ],
      "metadata": {
        "id": "XWdEiFEqk6ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 히스토그램 그리기\n",
        "for i , a in activations.items():\n",
        "  plt.subplot(1,len(activations), i+1)\n",
        "  plt.title(str(i+1) + \"layer\")\n",
        "  if i != 0 :\n",
        "    plt.yticks([],[])\n",
        "    plt.hist(a.flatten(), 30, range=(0,1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NKoiq8MtnALj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Xavier초깃값 - tanh함수"
      ],
      "metadata": {
        "id": "TFJOYHe7n0h0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def ReLU(x):\n",
        "  return np.maximum(0,x)\n",
        "  \n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "\n",
        "input_data = np.random.randn(1000,100)  # 1000개의 데이터 >> 정규분포로 무작위로 생성헤 5층 신경망에 흘림\n",
        "node_num = 100 # 각 은닉층의 노드(뉴런) 수\n",
        "hidden_layer_size = 5  # 은닉층이 5개\n",
        "activations = {}  # 이곳에 각 층의 활성화 결과를 저장\n",
        "\n",
        "x = input_data\n",
        "\n",
        "for i in range(hidden_layer_size):\n",
        "  if i != 0:\n",
        "    x = activations[i-1]\n",
        "\n",
        "  # 초깃값을 다양하게 바꿔가며 실험해보자!\n",
        "  w = np.random.randn(node_num, node_num) / np.sqrt(node_num)\n",
        "  # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
        "  # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
        "  a = np.dot(x,w)\n",
        "\n",
        "\n",
        "  #활성화 함수도 바꿔가며 실험해보자\n",
        "  # z = sigmoid(a)\n",
        "  # z = ReLU(a)\n",
        "  z = tanh(a)\n",
        "\n",
        "\n",
        "  activations[i] = z"
      ],
      "metadata": {
        "id": "XWD4N3z9nmSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 히스토그램 그리기\n",
        "for i , a in activations.items():\n",
        "  plt.subplot(1,len(activations), i+1)\n",
        "  plt.title(str(i+1) + \"layer\")\n",
        "  if i != 0 :\n",
        "    plt.yticks([],[])\n",
        "    plt.hist(a.flatten(), 30, range=(0,1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MytRtasKnpYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Xavier초깃값 - ReLU함수"
      ],
      "metadata": {
        "id": "n6_agw5YoDrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def ReLU(x):\n",
        "  return np.maximum(0,x)\n",
        "  \n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "\n",
        "input_data = np.random.randn(1000,100)  # 1000개의 데이터 >> 정규분포로 무작위로 생성헤 5층 신경망에 흘림\n",
        "node_num = 100 # 각 은닉층의 노드(뉴런) 수\n",
        "hidden_layer_size = 5  # 은닉층이 5개\n",
        "activations = {}  # 이곳에 각 층의 활성화 결과를 저장\n",
        "\n",
        "x = input_data\n",
        "\n",
        "for i in range(hidden_layer_size):\n",
        "  if i != 0:\n",
        "    x = activations[i-1]\n",
        "\n",
        "  # 초깃값을 다양하게 바꿔가며 실험해보자!\n",
        "  w = np.random.randn(node_num, node_num) / np.sqrt(node_num)\n",
        "  # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
        "  # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
        "  a = np.dot(x,w)\n",
        "\n",
        "\n",
        "  #활성화 함수도 바꿔가며 실험해보자\n",
        "  # z = sigmoid(a)\n",
        "  z = ReLU(a)\n",
        "  # z = tanh(a)\n",
        "\n",
        "\n",
        "  activations[i] = z"
      ],
      "metadata": {
        "id": "nPbhHuyBoEL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 히스토그램 그리기\n",
        "for i , a in activations.items():\n",
        "  plt.subplot(1,len(activations), i+1)\n",
        "  plt.title(str(i+1) + \"layer\")\n",
        "  if i != 0 :\n",
        "    plt.yticks([],[])\n",
        "    plt.hist(a.flatten(), 30, range=(0,1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fVsWMiaIoGIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# std = 0.01인 정규분포 & ReLU함수 사용"
      ],
      "metadata": {
        "id": "HAewkjkLq6oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def ReLU(x):\n",
        "  return np.maximum(0,x)\n",
        "  \n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "\n",
        "input_data = np.random.randn(1000,100)  # 1000개의 데이터 >> 정규분포로 무작위로 생성헤 5층 신경망에 흘림\n",
        "node_num = 100 # 각 은닉층의 노드(뉴런) 수\n",
        "hidden_layer_size = 5  # 은닉층이 5개\n",
        "activations = {}  # 이곳에 각 층의 활성화 결과를 저장\n",
        "\n",
        "x = input_data\n",
        "\n",
        "for i in range(hidden_layer_size):\n",
        "  if i != 0:\n",
        "    x = activations[i-1]\n",
        "\n",
        "  # 초깃값을 다양하게 바꿔가며 실험해보자!\n",
        "  w = np.random.randn(node_num, node_num) * 0.01\n",
        "  # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
        "  # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
        "  a = np.dot(x,w)\n",
        "\n",
        "\n",
        "  #활성화 함수도 바꿔가며 실험해보자\n",
        "  # z = sigmoid(a)\n",
        "  z = ReLU(a)\n",
        "  # z = tanh(a)\n",
        "\n",
        "\n",
        "  activations[i] = z"
      ],
      "metadata": {
        "id": "N-h_kWUgq5S7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 히스토그램 그리기\n",
        "for i , a in activations.items():\n",
        "  plt.subplot(1,len(activations), i+1)\n",
        "  plt.title(str(i+1) + \"layer\")\n",
        "  if i != 0 :\n",
        "    plt.yticks([],[])\n",
        "    plt.hist(a.flatten(), 30, range=(0,1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fl1tXTiXrSUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Xavier초깃값 & ReLU함수"
      ],
      "metadata": {
        "id": "PtP4YzGmrQkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def ReLU(x):\n",
        "  return np.maximum(0,x)\n",
        "  \n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "\n",
        "input_data = np.random.randn(1000,100)  # 1000개의 데이터 >> 정규분포로 무작위로 생성헤 5층 신경망에 흘림\n",
        "node_num = 100 # 각 은닉층의 노드(뉴런) 수\n",
        "hidden_layer_size = 5  # 은닉층이 5개\n",
        "activations = {}  # 이곳에 각 층의 활성화 결과를 저장\n",
        "\n",
        "x = input_data\n",
        "\n",
        "for i in range(hidden_layer_size):\n",
        "  if i != 0:\n",
        "    x = activations[i-1]\n",
        "\n",
        "  # 초깃값을 다양하게 바꿔가며 실험해보자!\n",
        "  w = np.random.randn(node_num, node_num) / np.sqrt(node_num)\n",
        "  # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
        "  # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
        "  a = np.dot(x,w)\n",
        "\n",
        "\n",
        "  #활성화 함수도 바꿔가며 실험해보자\n",
        "  # z = sigmoid(a)\n",
        "  z = ReLU(a)\n",
        "  # z = tanh(a)\n",
        "\n",
        "\n",
        "  activations[i] = z"
      ],
      "metadata": {
        "id": "RYqlJXYwr7MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 히스토그램 그리기\n",
        "for i , a in activations.items():\n",
        "  plt.subplot(1,len(activations), i+1)\n",
        "  plt.title(str(i+1) + \"layer\")\n",
        "  if i != 0 :\n",
        "    plt.yticks([],[])\n",
        "    plt.hist(a.flatten(), 30, range=(0,1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5Nt4L2sjsblB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# He 초깃값 & ReLU함수"
      ],
      "metadata": {
        "id": "i29BiJJhsSX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def ReLU(x):\n",
        "  return np.maximum(0,x)\n",
        "  \n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "\n",
        "input_data = np.random.randn(1000,100)  # 1000개의 데이터 >> 정규분포로 무작위로 생성헤 5층 신경망에 흘림\n",
        "node_num = 100 # 각 은닉층의 노드(뉴런) 수\n",
        "hidden_layer_size = 5  # 은닉층이 5개\n",
        "activations = {}  # 이곳에 각 층의 활성화 결과를 저장\n",
        "\n",
        "x = input_data\n",
        "\n",
        "for i in range(hidden_layer_size):\n",
        "  if i != 0:\n",
        "    x = activations[i-1]\n",
        "\n",
        "  # 초깃값을 다양하게 바꿔가며 실험해보자!\n",
        "  # w = np.random.randn(node_num, node_num) / np.sqrt(node_num)\n",
        "  # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
        "  w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
        "  a = np.dot(x,w)\n",
        "\n",
        "\n",
        "  #활성화 함수도 바꿔가며 실험해보자\n",
        "  # z = sigmoid(a)\n",
        "  z = ReLU(a)\n",
        "  # z = tanh(a)\n",
        "\n",
        "\n",
        "  activations[i] = z"
      ],
      "metadata": {
        "id": "xbo3UXrJsP8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 히스토그램 그리기\n",
        "for i , a in activations.items():\n",
        "  plt.subplot(1,len(activations), i+1)\n",
        "  plt.title(str(i+1) + \"layer\")\n",
        "  if i != 0 :\n",
        "    plt.yticks([],[])\n",
        "    plt.hist(a.flatten(), 30, range=(0,1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UCrB7UMUscef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MNIST 데이터셋으로 본 가중치 초깃값 비교"
      ],
      "metadata": {
        "id": "MSS5JstLtEEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.append(\"/content/drive/MyDrive/bottom/deep-learning-from-scratch-master/deep-learning-from-scratch-master/ch06\")  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.util import smooth_curve\n",
        "from common.multi_layer_net import MultiLayerNet\n",
        "from common.optimizer import SGD\n",
        "\n",
        "\n",
        "# 0. MNIST 데이터 읽기==========\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 128\n",
        "max_iterations = 2000\n",
        "\n",
        "\n",
        "# 1. 실험용 설정==========\n",
        "weight_init_types = {'std=0.01': 0.01, 'Xavier': 'sigmoid', 'He': 'relu'}\n",
        "optimizer = SGD(lr=0.01)\n",
        "\n",
        "networks = {}\n",
        "train_loss = {}\n",
        "for key, weight_type in weight_init_types.items():\n",
        "    networks[key] = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100],\n",
        "                                  output_size=10, weight_init_std=weight_type)\n",
        "    train_loss[key] = []\n",
        "\n",
        "\n",
        "# 2. 훈련 시작==========\n",
        "for i in range(max_iterations):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    for key in weight_init_types.keys():\n",
        "        grads = networks[key].gradient(x_batch, t_batch)\n",
        "        optimizer.update(networks[key].params, grads)\n",
        "    \n",
        "        loss = networks[key].loss(x_batch, t_batch)\n",
        "        train_loss[key].append(loss)\n",
        "    \n",
        "    if i % 100 == 0:\n",
        "        print(\"===========\" + \"iteration:\" + str(i) + \"===========\")\n",
        "        for key in weight_init_types.keys():\n",
        "            loss = networks[key].loss(x_batch, t_batch)\n",
        "            print(key + \":\" + str(loss))\n",
        "\n",
        "\n",
        "# 3. 그래프 그리기==========\n",
        "markers = {'std=0.01': 'o', 'Xavier': 's', 'He': 'D'}\n",
        "x = np.arange(max_iterations)\n",
        "for key in weight_init_types.keys():\n",
        "    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.ylim(0, 2.5)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "y36RWmOctJDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  배치 정규화의 효과"
      ],
      "metadata": {
        "id": "X8nC91sD-9yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(\"/content/drive/MyDrive/bottom/deep-learning-from-scratch-master/deep-learning-from-scratch-master/ch03\")  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
        "from common.optimizer import SGD, Adam\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "# 학습 데이터를 줄임\n",
        "x_train = x_train[:1000]\n",
        "t_train = t_train[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.01\n",
        "\n",
        "\n",
        "def __train(weight_init_std):\n",
        "    bn_network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10, \n",
        "                                    weight_init_std=weight_init_std, use_batchnorm=True)\n",
        "    network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10,\n",
        "                                weight_init_std=weight_init_std)\n",
        "    optimizer = SGD(lr=learning_rate)\n",
        "    \n",
        "    train_acc_list = []\n",
        "    bn_train_acc_list = []\n",
        "    \n",
        "    iter_per_epoch = max(train_size / batch_size, 1)\n",
        "    epoch_cnt = 0\n",
        "    \n",
        "    for i in range(1000000000):\n",
        "        batch_mask = np.random.choice(train_size, batch_size)\n",
        "        x_batch = x_train[batch_mask]\n",
        "        t_batch = t_train[batch_mask]\n",
        "    \n",
        "        for _network in (bn_network, network):\n",
        "            grads = _network.gradient(x_batch, t_batch)\n",
        "            optimizer.update(_network.params, grads)\n",
        "    \n",
        "        if i % iter_per_epoch == 0:\n",
        "            train_acc = network.accuracy(x_train, t_train)\n",
        "            bn_train_acc = bn_network.accuracy(x_train, t_train)\n",
        "            train_acc_list.append(train_acc)\n",
        "            bn_train_acc_list.append(bn_train_acc)\n",
        "    \n",
        "            print(\"epoch:\" + str(epoch_cnt) + \" | \" + str(train_acc) + \" - \" + str(bn_train_acc))\n",
        "    \n",
        "            epoch_cnt += 1\n",
        "            if epoch_cnt >= max_epochs:\n",
        "                break\n",
        "                \n",
        "    return train_acc_list, bn_train_acc_list\n",
        "\n",
        "\n",
        "# 그래프 그리기==========\n",
        "weight_scale_list = np.logspace(0, -4, num=16)\n",
        "x = np.arange(max_epochs)\n",
        "\n",
        "for i, w in enumerate(weight_scale_list):\n",
        "    print( \"============== \" + str(i+1) + \"/16\" + \" ==============\")\n",
        "    train_acc_list, bn_train_acc_list = __train(w)\n",
        "    \n",
        "    plt.subplot(4,4,i+1)\n",
        "    plt.title(\"W:\" + str(w))\n",
        "    if i == 15:\n",
        "        plt.plot(x, bn_train_acc_list, label='Batch Normalization', markevery=2)\n",
        "        plt.plot(x, train_acc_list, linestyle = \"--\", label='Normal(without BatchNorm)', markevery=2)\n",
        "    else:\n",
        "        plt.plot(x, bn_train_acc_list, markevery=2)\n",
        "        plt.plot(x, train_acc_list, linestyle=\"--\", markevery=2)\n",
        "\n",
        "    plt.ylim(0, 1.0)\n",
        "    if i % 4:\n",
        "        plt.yticks([])\n",
        "    else:\n",
        "        plt.ylabel(\"accuracy\")\n",
        "    if i < 12:\n",
        "        plt.xticks([])\n",
        "    else:\n",
        "        plt.xlabel(\"epochs\")\n",
        "    plt.legend(loc='lower right')\n",
        "    \n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SoaznX2N-7Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 오버피팅 일으키기"
      ],
      "metadata": {
        "id": "J8myYuCUAKr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.append(\"/content/drive/MyDrive/bottom/deep-learning-from-scratch-master/deep-learning-from-scratch-master/ch03\")  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.multi_layer_net import MultiLayerNet\n",
        "from common.optimizer import SGD\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
        "x_train = x_train[:300]\n",
        "t_train = t_train[:300]\n",
        "\n",
        "# weight decay（가중치 감쇠） 설정 =======================\n",
        "#weight_decay_lambda = 0 # weight decay를 사용하지 않을 경우\n",
        "weight_decay_lambda = 0.1\n",
        "# ====================================================\n",
        "\n",
        "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
        "                        weight_decay_lambda=weight_decay_lambda)\n",
        "optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신\n",
        "\n",
        "max_epochs = 201\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "epoch_cnt = 0\n",
        "\n",
        "for i in range(1000000000):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    grads = network.gradient(x_batch, t_batch)\n",
        "    optimizer.update(network.params, grads)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "\n",
        "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
        "\n",
        "        epoch_cnt += 1\n",
        "        if epoch_cnt >= max_epochs:\n",
        "            break\n",
        "\n",
        "\n",
        "# 그래프 그리기==========\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VqIJ2k5-AMyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 가중치 감소"
      ],
      "metadata": {
        "id": "M1OkbAc6Blpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # coding: utf-8\n",
        "# import os\n",
        "# import sys\n",
        "\n",
        "# sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from dataset.mnist import load_mnist\n",
        "# from common.multi_layer_net import MultiLayerNet\n",
        "# from common.optimizer import SGD\n",
        "\n",
        "\n",
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(\"/content/drive/MyDrive/bottom/deep-learning-from-scratch-master/deep-learning-from-scratch-master/ch03\")  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "\n",
        "class MultiLayerNet:\n",
        "    \"\"\"완전연결 다층 신경망\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
        "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
        "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
        "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
        "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
        "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
        "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
        "    weight_decay_lambda : 가중치 감소(L2 법칙)의 세기\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size_list, output_size,\n",
        "                 activation='relu', weight_init_std='relu', weight_decay_lambda=0):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size_list = hidden_size_list\n",
        "        self.hidden_layer_num = len(hidden_size_list)\n",
        "        self.weight_decay_lambda = weight_decay_lambda\n",
        "        self.params = {}\n",
        "\n",
        "        # 가중치 초기화\n",
        "        self.__init_weight(weight_init_std)\n",
        "\n",
        "        # 계층 생성\n",
        "        activation_layer = {'sigmoid': Sigmoid, 'relu': Relu}\n",
        "        self.layers = OrderedDict()\n",
        "        for idx in range(1, self.hidden_layer_num+1):\n",
        "            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\n",
        "                                                      self.params['b' + str(idx)])\n",
        "            self.layers['Activation_function' + str(idx)] = activation_layer[activation]()\n",
        "\n",
        "        idx = self.hidden_layer_num + 1\n",
        "        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\n",
        "            self.params['b' + str(idx)])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def __init_weight(self, weight_init_std):\n",
        "        \"\"\"가중치 초기화\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
        "            'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
        "            'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
        "        \"\"\"\n",
        "        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
        "        for idx in range(1, len(all_size_list)):\n",
        "            scale = weight_init_std\n",
        "            if str(weight_init_std).lower() in ('relu', 'he'):\n",
        "                scale = np.sqrt(2.0 / all_size_list[idx - 1])  # ReLU를 사용할 때의 권장 초깃값\n",
        "            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):\n",
        "                scale = np.sqrt(1.0 / all_size_list[idx - 1])  # sigmoid를 사용할 때의 권장 초깃값\n",
        "            self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])\n",
        "            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        \"\"\"손실 함수를 구한다.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블 \n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        손실 함수의 값\n",
        "        \"\"\"\n",
        "        y = self.predict(x)\n",
        "\n",
        "        weight_decay = 0\n",
        "        for idx in range(1, self.hidden_layer_num + 2):\n",
        "            W = self.params['W' + str(idx)]\n",
        "            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W ** 2)\n",
        "\n",
        "        return self.last_layer.forward(y, t) + weight_decay\n",
        "\n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다(수치 미분).\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 딕셔너리(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in range(1, self.hidden_layer_num+2):\n",
        "            grads['W' + str(idx)] = numerical_gradient(loss_W, self.params['W' + str(idx)])\n",
        "            grads['b' + str(idx)] = numerical_gradient(loss_W, self.params['b' + str(idx)])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다(오차역전파법).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 딕셔너리(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        for idx in range(1, self.hidden_layer_num+2):\n",
        "            grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.layers['Affine' + str(idx)].W\n",
        "            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\n",
        "\n",
        "        return grads\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
        "x_train = x_train[:300]\n",
        "t_train = t_train[:300]\n",
        "\n",
        "# weight decay（가중치 감쇠） 설정 =======================\n",
        "#weight_decay_lambda = 0 # weight decay를 사용하지 않을 경우\n",
        "weight_decay_lambda = 0.1\n",
        "# ====================================================\n",
        "\n",
        "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
        "                        weight_decay_lambda=weight_decay_lambda)\n",
        "optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신\n",
        "\n",
        "max_epochs = 201\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "epoch_cnt = 0\n",
        "\n",
        "for i in range(1000000000):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    grads = network.gradient(x_batch, t_batch)\n",
        "    optimizer.update(network.params, grads)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "\n",
        "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
        "\n",
        "        epoch_cnt += 1\n",
        "        if epoch_cnt >= max_epochs:\n",
        "            break\n",
        "\n",
        "\n",
        "# 그래프 그리기==========\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NEEscz9RBpdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 드롭아웃"
      ],
      "metadata": {
        "id": "t1Wmw2c0DBqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dropout:\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "     self.dropout_ration = dropout_ratio\n",
        "     self.mask = None\n",
        "\n",
        "    def forward(self, x, train_flg = True):\n",
        "      if train_flg:\n",
        "        self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "        return x*self.mask\n",
        "      else:\n",
        "        return x * (1.0 - self.dropout_ratio)\n",
        "        \n",
        "    def backward(self, dout):\n",
        "      return dout * self.mask\n"
      ],
      "metadata": {
        "id": "_JozUFEBDDOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(\"/content/drive/MyDrive/bottom/deep-learning-from-scratch-master/deep-learning-from-scratch-master/ch03\")  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "from common.optimizer import *\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"신경망 훈련을 대신 해주는 클래스\n",
        "    \"\"\"\n",
        "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
        "                 epochs=20, mini_batch_size=100,\n",
        "                 optimizer='SGD', optimizer_param={'lr':0.01}, \n",
        "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
        "        self.network = network\n",
        "        self.verbose = verbose\n",
        "        self.x_train = x_train\n",
        "        self.t_train = t_train\n",
        "        self.x_test = x_test\n",
        "        self.t_test = t_test\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = mini_batch_size\n",
        "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
        "\n",
        "        # optimzer\n",
        "        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\n",
        "                                'adagrad':AdaGrad, 'rmsprpo':RMSprop, 'adam':Adam}\n",
        "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
        "        \n",
        "        self.train_size = x_train.shape[0]\n",
        "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
        "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
        "        self.current_iter = 0\n",
        "        self.current_epoch = 0\n",
        "        \n",
        "        self.train_loss_list = []\n",
        "        self.train_acc_list = []\n",
        "        self.test_acc_list = []\n",
        "\n",
        "    def train_step(self):\n",
        "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
        "        x_batch = self.x_train[batch_mask]\n",
        "        t_batch = self.t_train[batch_mask]\n",
        "        \n",
        "        grads = self.network.gradient(x_batch, t_batch)\n",
        "        self.optimizer.update(self.network.params, grads)\n",
        "        \n",
        "        loss = self.network.loss(x_batch, t_batch)\n",
        "        self.train_loss_list.append(loss)\n",
        "        if self.verbose: print(\"train loss:\" + str(loss))\n",
        "        \n",
        "        if self.current_iter % self.iter_per_epoch == 0:\n",
        "            self.current_epoch += 1\n",
        "            \n",
        "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
        "            x_test_sample, t_test_sample = self.x_test, self.t_test\n",
        "            if not self.evaluate_sample_num_per_epoch is None:\n",
        "                t = self.evaluate_sample_num_per_epoch\n",
        "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
        "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
        "                \n",
        "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
        "            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
        "            self.train_acc_list.append(train_acc)\n",
        "            self.test_acc_list.append(test_acc)\n",
        "\n",
        "            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
        "        self.current_iter += 1\n",
        "\n",
        "    def train(self):\n",
        "        for i in range(self.max_iter):\n",
        "            self.train_step()\n",
        "\n",
        "        test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"=============== Final Test Accuracy ===============\")\n",
        "            print(\"test acc:\" + str(test_acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "ZTF5M-B7DBL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 적절한 하이퍼파라미터 값 찾기"
      ],
      "metadata": {
        "id": "gaX-UBZcE5DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def smooth_curve(x):\n",
        "    \"\"\"손실 함수의 그래프를 매끄럽게 하기 위해 사용\n",
        "    \n",
        "    참고：http://glowingpython.blogspot.jp/2012/02/convolution-with-numpy.html\n",
        "    \"\"\"\n",
        "    window_len = 11\n",
        "    s = np.r_[x[window_len-1:0:-1], x, x[-1:-window_len:-1]]\n",
        "    w = np.kaiser(window_len, 2)\n",
        "    y = np.convolve(w/w.sum(), s, mode='valid')\n",
        "    return y[5:len(y)-5]\n",
        "\n",
        "\n",
        "def shuffle_dataset(x, t):\n",
        "    \"\"\"데이터셋을 뒤섞는다.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : 훈련 데이터\n",
        "    t : 정답 레이블\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    x, t : 뒤섞은 훈련 데이터와 정답 레이블\n",
        "    \"\"\"\n",
        "    permutation = np.random.permutation(x.shape[0])\n",
        "    x = x[permutation,:] if x.ndim == 2 else x[permutation,:,:,:]\n",
        "    t = t[permutation]\n",
        "\n",
        "    return x, t\n",
        "\n",
        "def conv_output_size(input_size, filter_size, stride=1, pad=0):\n",
        "    return (input_size + 2*pad - filter_size) / stride + 1\n",
        "\n",
        "\n",
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
        "    filter_h : 필터의 높이\n",
        "    filter_w : 필터의 너비\n",
        "    stride : 스트라이드\n",
        "    pad : 패딩\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    col : 2차원 배열\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_data.shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "\n",
        "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
        "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "\n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
        "    return col\n",
        "\n",
        "\n",
        "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    col : 2차원 배열(입력 데이터)\n",
        "    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n",
        "    filter_h : 필터의 높이\n",
        "    filter_w : 필터의 너비\n",
        "    stride : 스트라이드\n",
        "    pad : 패딩\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    img : 변환된 이미지들\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
        "\n",
        "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
        "\n",
        "    return img[:, :, pad:H + pad, pad:W + pad]\n"
      ],
      "metadata": {
        "id": "ZOFf4eqsE9Nz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
