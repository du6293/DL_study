{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5장.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPMlRg3blHq50PoOH1/SEBF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### 단순한 계층 구현하기\n","계층 : 신경망의 기능 단위\n","\n"],"metadata":{"id":"U6ucN1ZCPQbr"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"Xg9qxpnlFzbn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["곱셈계층"],"metadata":{"id":"yVKQIN-U5787"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"z4xLAcTLDO54"},"outputs":[],"source":["\n","class MulLayer:\n","  def __init__(self):\n","    self.x = None\n","    self.y = None\n","\n","  def forward(self,x,y):  # 순전파\n","    self.x = x\n","    self.y = y\n","    out = x * y\n","\n","    return out\n","\n","  def backward(self, dout):  # 역전파\n","    dx = dout * self.y  # x와 y를 바꾼다\n","    dy = dout * self.x\n","\n","    return dx, dy\n"]},{"cell_type":"code","source":["apple = 100\n","apple_num = 2\n","tax = 1.1\n","\n","#계층들\n","mul_apple_layer = MulLayer()\n","mul_tax_layer = MulLayer()\n","\n","#순전파\n","apple_price = mul_apple_layer.forward(apple, apple_num)\n","price = mul_tax_layer.forward(apple_price, tax)\n","\n","print(price) # 220\n","\n","# 역전파\n","dprice = 1\n","dapple_price,dtax = mul_tax_layer.backward(dprice)\n","dapple,dapple_num = mul_apple_layer.backward(dapple_price)\n","\n","print(dapple, dapple_num, dtax) # 2.2 110 200"],"metadata":{"id":"Ev39img9QXDg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 뎃셈계층"],"metadata":{"id":"rZ4lULB7RThy"}},{"cell_type":"code","source":["class AddLayer:\n","  def __init__(self):\n","    pass  # 덧셈계층에서는 초기화 필요x\n","\n","  def forward(self, x, y):\n","    out = x + y\n","    return out\n","\n","  def backward(self, dout):\n","    dx = dout * 1\n","    dy = dout * 1\n","    return dx, dy"],"metadata":{"id":"TuGwKdBNRRHy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["apple = 100\n","apple_num = 2\n","orange = 150\n","orange_num = 3\n","tax = 1.1\n","\n","# 계층들\n","mul_apple_layer = MulLayer()\n","mul_orange_layer = MulLayer()\n","add_apple_orange_layer = AddLayer()\n","mul_tax_layer = MulLayer()\n","\n","# 순전파\n","\n","apple_price = mul_apple_layer.fowrard(apple,apple_num)\n","orange_price = mul_orange_layer.forward(orange, orange_num)\n","all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n","price = mul_tax_layer.forward(all_price, tax)\n","\n","# 역전파\n","dprice = 1\n","dall_price,dtax = mul_tax_layer.backward(dprice)\n","dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n","dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n","dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n","\n","price(price)\n","print(dapple_num, dapple, dorange, dorange_num, dtax)"],"metadata":{"id":"fx_DfovJ6EXn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###  활성화 함수 계층 구현하기"],"metadata":{"id":"Rvw5ptBX8mE4"}},{"cell_type":"markdown","source":["ReLU 계층"],"metadata":{"id":"HEpMZswv80xz"}},{"cell_type":"code","source":["class Relu:\n","  def __init__(self):\n","    self.mask = None\n","  \n","  def forward(self,x):\n","    self.mask = (x <= 0)  # mask는 True/False로 구성된 넘파이 배열. x <=0 이면 True, x>0이면 False\n","    out = x.copy()\n","    out[self.maxk] = 0\n","\n","    return out\n","\n","  def backward(self, dout):\n","    dout[self.mask] = 0\n","    dx = dout\n","  \n","    return dx"],"metadata":{"id":"raoO9gTQ8zyR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["sigmoid계층"],"metadata":{"id":"vyLnLC3D_u6d"}},{"cell_type":"code","source":["class Sigmoid:\n","  def __init__(self):\n","    self.out = None\n","\n","  def forward(self,x):\n","    out = 1 / (1+ np.exp(-x))\n","    self.out = out\n","  def backward(self, dout):\n","    dx = dout * (1.0 - self.out) * self.out\n","\n","    return dx"],"metadata":{"id":"1RbyCQRi8l08"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Affine/Softmax 계층 구현하기"],"metadata":{"id":"qXby1_qj_7uU"}},{"cell_type":"markdown","source":["Affine계층"],"metadata":{"id":"ENkpEa7YAEh_"}},{"cell_type":"markdown","source":["배치용 Affine계층"],"metadata":{"id":"2NuWUNXVFaIo"}},{"cell_type":"code","source":["class Affine:\n","  def __init__(self, w, b):\n","    self.w = w\n","    self.b = b\n","    self.x = None\n","    self.dw = None\n","    self.db = None\n","\n","  def forward(self, x):\n","    self.x = x\n","    out = np.dot(x,self.w) + self.b\n","\n","  def backward(self, dout):\n","    dx = np.dot(dout,self.w.t)\n","    self.dw = np.dot(self.x.t, dout)\n","    self.db = np.sum(dout, axis = 0)\n","\n","    return dx"],"metadata":{"id":"T3pxu-wF_6rd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Softmax - with - Loss 계층"],"metadata":{"id":"53Zy8HJ5HJ8o"}},{"cell_type":"code","source":["# 개선된 softmax함수\n","def softmax(a):\n","  c = np.max(a)\n","  exp_a = np.exp(a-c)  # 오버플로 대책\n","  sum_exp_a = np.sum(exp_a)\n","  y = exp_a / sum_exp_a\n","\n","  return y"],"metadata":{"id":"NQXo7fiTK84l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#(배치용) 교차 엔트로피 오차 구현(정답 레이블 t가 one-hot-encoding일 때)\n","def cross_entropy_error(y,t):  # y: 신경망의 출력, t : 정답 레이블\n","  if y.ndim == 1:\n","    t = t.reshape(1,t.size)  # reshape : 데이터의 형상을 바꿈\n","    y = y.reshape(1,y.size)\n","  batch_size = y.shape[0]\n","  return -np.sum(t*np.log(y+1e-7)) / batch_size  # 배치의 크기로 나누어 정규화한 후 1장당 평균의 교차 엔트로피 오차를 계산"],"metadata":{"id":"QWWOSh1XK9vq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SoftmaxWithLoss:\n","  def __init__ (self):\n","    self.loss = None  # 손실\n","    self.y = None  # softmax의 출력\n","    self.t = None  # 정답 레이블(원-핫 벡터)\n","\n","  def forward(self, x, t):\n","    self.t = t\n","    self.y = softmax(x)\n","    self.loss = cross_entropy_error(self,y,self.t)\n","    return self.loss\n","  def backward(self,dout = 1):\n","    batch_size = self.t.shape[0]\n","    ds = (self.y - self.t) / batch_size"],"metadata":{"id":"3ddcZNkSHPoM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 오차역전파법 구현하기"],"metadata":{"id":"SgC6o7UbL_Ne"}},{"cell_type":"markdown","source":["## 신경망의 전체 그림\n","전제) 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련데이터에 적응하도록 조정하는 과정을 '학습'이라고 한다.\n","\n","1단계 - 미니배치\n","훈련 데이터 중 일부를 무작위로 가져온다. > 미니배치\n","목표 : 미니배치의 손실함수값을 줄이는 것\n","\n","2단게 - 기울기 산출\n","미니배치함수의 손실함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다.\n","기울기는 손실함수의 값을 가장 작게 하는 방향을 제시한다.\n","\n","3단계 - 매개변수 갱신 \n","가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n","\n","4단계 - 반복\n","1~3단계를 반복\n"],"metadata":{"id":"9xMtqcqQQPPt"}},{"cell_type":"code","source":["import os , sys\n","sys.path.append()\n","import numpy as np\n","from common.layers import *\n","from common.gradient import numerical_gradient\n","from collections import OrderedDict\n","\n","class TwoLayerNet:\n","  def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n","    # 가중치 초기화\n","    self.params = {}\n","    self.params = ['w1'] = weight_init_std * np.random.radn(input_size, hidden_size)\n","    self.params = ['b1'] = np.zeros(hidden_size)\n","    self.params = ['w2'] = weight_init_std * np.randmo.randn(hidden_size, output_size)\n","    self.params = ['b2'] = np.zeros(output_size)\n","\n","    # 계층 생성\n","    self.layers = OrderedDict()\n","    self.layers['Affine1'] = Affine(self.params['w1'], self.params['b1'])\n","    self.layers['Relu1'] = Relu()\n","    self.layers['Affine2'] = Affine(self.params['w2'],self.params['b2'])\n","\n","    self.lastLayer = SoftmaxWithLoss()\n","\n","  def predict(self,x):\n","    for layer in self.layers.values():\n","      x = layer.forward()\n","    return x\n","\n","  # x : 입력 데이터, t : 정답 레이블\n","\n","  def loss(self, x, t):\n","    y = self.predict(x)\n","    return self.lastLayer.forward(y,t)\n","\n","  def accuracy(self,x,t):\n","    y = self.predict(x)\n","    y = np.argmax(y, axis = 1)\n","    if t.ndim != 1:\n","      t = np.argmax(t, axis = 1)\n","\n","      accuracy = np.sum(y == t) / float(x.shape[0])\n","    return accuracy\n","\n","\n","    # x: 입력 데이터, t: 정답 레이블\n","    def numerical_gradient(self, x, t):\n","      loss_w = lambda w:self.loss(x,t)\n","\n","      grads = {}\n","      grads['w1'] = numerical_gradient(loss_w, self.params['w1'])\n","      grads['b1'] = numerical_gradient(loss_w, self.params['b1'])\n","      grads['w2'] = numerical_gradient(loss_w, self.params['w2'])\n","      grads['b2'] = numerical_gradient(loss_w, self.params['b2'])\n","      return grads\n","\n","    def gradient(self,x,t):\n","      # 순전파\n","      self.loss(x,t)\n","\n","      # 역전파\n","      dout = 1\n","      dout = self.lastLayer.backward(dout)\n","\n","      layers = list(self.layers.valuees())\n","      laysers.reverse()\n","      for layer in layers:\n","        dout = layer.backward(dout)\n","\n","      # 결과 저장\n","      grads = {}\n","      grads['w1'] = self.layers['Affine1'].dw\n","      grads['b1'] = self.layers['Affine1'].db\n","      grads['w2'] = self.layers['Affine2'].dw\n","      grads['b2'] = self.layers['Affine2'].db\n","\n","      return grads"],"metadata":{"id":"MCYDapwLL8IL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 오차 역전파법으로 구한 기울기 검증하기"],"metadata":{"id":"b_hrfLQlYQyR"}},{"cell_type":"code","source":["import sys, os\n","sys.path.append()\n","impor numpy as np\n","from dataset.mnist import load_mnist\n","from two_layer_net import TwoLayerNet\n","\n","# 데이터 읽기\n","(x_train, t_train),(x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n","\n","network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n","\n","x_batch = x_train[:3]\n","t_batch = t_train[:3]\n","\n","grad_numerical = network.numerical_gradient(x_batch, t_batch)\n","grad_backprop = network.gradient(x_batch, t_batch)\n","\n","# 각 가중치의 차이의 절댓값을 구한후, 그 절댓값들의 평균을 낸다.\n","for key in grad_numerical.keys():\n","  diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n","  print(key + \".\" + str(diff))\n","  "],"metadata":{"id":"siTfUuZ5YPkk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 오차역전파법을 사용한 학습 구현하기"],"metadata":{"id":"D0X-kBeHalR_"}},{"cell_type":"code","source":["import sys, os\n","sys.path.append()\n","import numpy as np\n","from dataset.mnist imoprt load_mnist\n","from two_layer_net import TwoLayerNet\n","\n","# 데이터 읽기\n","(x_train,t_train),(x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n","network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n","\n","iters_num = 10000\n","train_size = x_train.shape[0]\n","batch_size = 100\n","learning_rate = 0.1\n","\n","train_loss_list = []\n","train_acc_list = []\n","test_acc_list = []\n","\n","iter_per_epoch = max(train_size / batch_size, 1)\n","\n","for i in range(iters_num):\n","  batch_mask = np.random.choice(train_size, batch_size)\n","  x_batch = x_train[batch_mask]\n","  t_batch = t_train[batch_mask]\n","\n","  # 오차역전파법으로 기울기를 구한다.\n","  grad = network.gradient(x_batch, t_batch)\n","\n","  #갱신\n","  for key in('w1','b1','w2','b2'):\n","    network.params[key] -= learning_rate * grad[key]\n","\n","  loss = network.loss(x_batch, t_batch)\n","  train_loss_list.append(loss)\n","\n","  if i % iter_per_epoch == 0:\n","    train_acc = network.accuracy(x_train, t_train)\n","    test_acc = network.accuracy(x_test, t_test)\n","    train_acc_list.append(train_acc)\n","    test_acc_list.append(test_acc)\n","    print(train_acc, test_acc)"],"metadata":{"id":"WzdhGw1Napaj"},"execution_count":null,"outputs":[]}]}